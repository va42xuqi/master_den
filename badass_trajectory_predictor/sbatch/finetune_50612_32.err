/data/beegfs/home/gosalcds/miniconda3/envs/paper/lib/python3.11/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
wandb: Currently logged in as: luffnis (luffnis1). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in ./wandb/run-20240727_064123-brepm159
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pos_lstm_SOC_train_50_100
wandb: â­ï¸ View project at https://wandb.ai/luffnis1/lightning_logs
wandb: ğŸš€ View run at https://wandb.ai/luffnis1/lightning_logs/runs/brepm159
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name          | Type    | Params
------------------------------------------
0 | preprocessing | LMU     | 199 K 
1 | dropout_layer | Dropout | 0     
2 | lstm          | LSTM    | 526 K 
3 | fc_out        | Linear  | 51.4 K
------------------------------------------
777 K     Trainable params
0         Non-trainable params
777 K     Total params
3.109     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val/ADE improved. New best score: 2.485
Epoch 0, global step 95: 'val/ADE' reached 2.48546 (best 2.48546), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/pos_lstm_nba_fine_tune.ckpt' as top 1
Epoch 1, global step 190: 'val/ADE' was not in top 1
Epoch 2, global step 285: 'val/ADE' was not in top 1
Epoch 3, global step 380: 'val/ADE' was not in top 1
Epoch 4, global step 475: 'val/ADE' was not in top 1
Epoch 5, global step 570: 'val/ADE' was not in top 1
Epoch 6, global step 665: 'val/ADE' was not in top 1
Epoch 7, global step 760: 'val/ADE' was not in top 1
Epoch 8, global step 855: 'val/ADE' was not in top 1
Epoch 9, global step 950: 'val/ADE' was not in top 1
Epoch 10, global step 1045: 'val/ADE' was not in top 1
Epoch 11, global step 1140: 'val/ADE' was not in top 1
Epoch 12, global step 1235: 'val/ADE' was not in top 1
Epoch 13, global step 1330: 'val/ADE' was not in top 1
Epoch 14, global step 1425: 'val/ADE' was not in top 1
Epoch 15, global step 1520: 'val/ADE' was not in top 1
Epoch 16, global step 1615: 'val/ADE' was not in top 1
Monitored metric val/ADE = nan is not finite. Previous best value was 2.485. Signaling Trainer to stop.
Epoch 17, global step 1710: 'val/ADE' was not in top 1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
SLURM auto-requeueing enabled. Setting signal handlers.
wandb: - 8.918 MB of 8.918 MB uploadedwandb: \ 8.918 MB of 8.947 MB uploadedwandb: | 8.947 MB of 8.947 MB uploadedwandb: 
wandb: Run history:
wandb:   CPU energy consumption (kWh) â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–
wandb:   GPU energy consumption (kWh) â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–
wandb:   RAM energy consumption (kWh) â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–
wandb: Total energy consumption (kWh) â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–
wandb:                          epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:               train/loss_epoch â–‚â–â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–†â–‡â–‡ 
wandb:                train/loss_step â–‚â–‚â–‚â–ƒâ–‚â–â–…â–‡â–‡â–‡â–…â–…â–…â–…â–…â–…â–†â–„â–„â–‡â–ƒâ–„â–‡â–…â–‡â–„â–„â–†â–…â–†â–„â–†â–…â–„â–‚â–…â–†â–ƒâ–…â–ˆ
wandb:            trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–â–ƒâ–â–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–â–†â–â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  val/ADE_epoch â–â–â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡  
wandb:                   val/ADE_step â–‚â–â–ƒâ–‚â–ƒâ–…â–†â–‡â–†â–ˆâ–ˆâ–‡â–‡â–‡â–†â–‡â–…â–†â–…â–‡â–†â–†â–†â–…â–„â–‡â–„â–†â–†â–‡â–„â–‡â–…â–‡â–‡â–‡    
wandb:                  val/FDE_epoch â–â–‚â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–†â–‡â–ˆâ–ˆâ–†â–ˆâ–ˆâ–‡â–‡â–‡  
wandb:                   val/FDE_step â–ƒâ–â–„â–‚â–„â–…â–…â–‡â–…â–‡â–ˆâ–‡â–†â–†â–†â–‡â–„â–…â–„â–†â–…â–†â–…â–„â–ƒâ–‡â–ƒâ–…â–…â–‡â–ƒâ–‡â–„â–†â–‡â–†    
wandb:               val/NL_ADE_epoch â–â–â–ˆâ–†â–‡â–‡â–ˆâ–†â–†â–‡â–ˆâ–‡â–‡â–ˆâ–†â–‡â–‡  
wandb:                val/NL_ADE_step â–ƒâ–â–ƒâ–â–ƒâ–…â–†â–‡â–†â–‡â–‡â–†â–†â–‡â–…â–‡â–…â–…â–…â–†â–…â–†â–…â–†â–„â–‡â–…â–†â–†â–ˆâ–„â–†â–…â–ˆâ–ˆâ–‡    
wandb: 
wandb: Run summary:
wandb:   CPU energy consumption (kWh) 3e-05
wandb:   GPU energy consumption (kWh) 0.00014
wandb:   RAM energy consumption (kWh) 0.0
wandb: Total energy consumption (kWh) 0.00016
wandb:                          epoch 18
wandb:               train/loss_epoch nan
wandb:                train/loss_step nan
wandb:            trainer/global_step 1710
wandb:                  val/ADE_epoch nan
wandb:                   val/ADE_step nan
wandb:                  val/FDE_epoch nan
wandb:                   val/FDE_step nan
wandb:               val/NL_ADE_epoch nan
wandb:                val/NL_ADE_step nan
wandb: 
wandb: ğŸš€ View run pos_lstm_SOC_train_50_100 at: https://wandb.ai/luffnis1/lightning_logs/runs/brepm159
wandb: â­ï¸ View project at: https://wandb.ai/luffnis1/lightning_logs
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_064123-brepm159/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
