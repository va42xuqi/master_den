/data/beegfs/home/gosalcds/miniconda3/envs/paper/lib/python3.11/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
wandb: Currently logged in as: luffnis (luffnis1). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in ./wandb/run-20240726_163207-mg1gx42r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run uni_lstm_SOC_train_50_100
wandb: â­ï¸ View project at https://wandb.ai/luffnis1/lightning_logs
wandb: ğŸš€ View run at https://wandb.ai/luffnis1/lightning_logs/runs/mg1gx42r
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name            | Type   | Params
-------------------------------------------
0 | x_preprocessing | LMU    | 199 K 
1 | y_preprocessing | LMU    | 199 K 
2 | x_lstm          | LSTM   | 526 K 
3 | y_lstm          | LSTM   | 526 K 
4 | x_fc_out        | Linear | 25.7 K
5 | y_fc_out        | Linear | 25.7 K
-------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.012     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
wandb: 429 encountered (Filestream rate limit exceeded, retrying in 2.0 seconds.), retrying request
wandb: 429 encountered (Filestream rate limit exceeded, retrying in 4.3 seconds.), retrying request
wandb: 429 encountered (Filestream rate limit exceeded, retrying in 2.0 seconds.), retrying request
wandb: 429 encountered (Filestream rate limit exceeded, retrying in 4.3 seconds.), retrying request
Metric val/ADE improved. New best score: 2.947
Epoch 0, global step 83: 'val/ADE' reached 2.94729 (best 2.94729), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_pretrain.ckpt' as top 1
Metric val/ADE improved by 0.196 >= min_delta = 0.0. New best score: 2.751
Epoch 1, global step 166: 'val/ADE' reached 2.75145 (best 2.75145), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_pretrain.ckpt' as top 1
Metric val/ADE improved by 0.016 >= min_delta = 0.0. New best score: 2.735
Epoch 2, global step 249: 'val/ADE' reached 2.73515 (best 2.73515), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_pretrain.ckpt' as top 1
Epoch 3, global step 332: 'val/ADE' was not in top 1
Epoch 4, global step 415: 'val/ADE' was not in top 1
Metric val/ADE improved by 0.031 >= min_delta = 0.0. New best score: 2.704
Epoch 5, global step 498: 'val/ADE' reached 2.70432 (best 2.70432), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_pretrain.ckpt' as top 1
Epoch 6, global step 581: 'val/ADE' was not in top 1
Metric val/ADE improved by 0.021 >= min_delta = 0.0. New best score: 2.683
Epoch 7, global step 664: 'val/ADE' reached 2.68296 (best 2.68296), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_pretrain.ckpt' as top 1
Metric val/ADE improved by 0.012 >= min_delta = 0.0. New best score: 2.671
Epoch 8, global step 747: 'val/ADE' reached 2.67092 (best 2.67092), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_pretrain.ckpt' as top 1
Epoch 9, global step 830: 'val/ADE' was not in top 1
Epoch 10, global step 913: 'val/ADE' was not in top 1
Metric val/ADE improved by 0.037 >= min_delta = 0.0. New best score: 2.634
Epoch 11, global step 996: 'val/ADE' reached 2.63388 (best 2.63388), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_pretrain.ckpt' as top 1
Metric val/ADE improved by 0.107 >= min_delta = 0.0. New best score: 2.527
Epoch 12, global step 1079: 'val/ADE' reached 2.52679 (best 2.52679), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_pretrain.ckpt' as top 1
Epoch 13, global step 1162: 'val/ADE' was not in top 1
Epoch 14, global step 1245: 'val/ADE' was not in top 1
Epoch 15, global step 1328: 'val/ADE' was not in top 1
Epoch 16, global step 1411: 'val/ADE' was not in top 1
Epoch 17, global step 1494: 'val/ADE' was not in top 1
Epoch 18, global step 1577: 'val/ADE' was not in top 1
Epoch 19, global step 1660: 'val/ADE' was not in top 1
Epoch 20, global step 1743: 'val/ADE' was not in top 1
Epoch 21, global step 1826: 'val/ADE' was not in top 1
Epoch 22, global step 1909: 'val/ADE' was not in top 1
Epoch 23, global step 1992: 'val/ADE' was not in top 1
Metric val/ADE improved by 0.023 >= min_delta = 0.0. New best score: 2.504
Epoch 24, global step 2075: 'val/ADE' reached 2.50373 (best 2.50373), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_pretrain.ckpt' as top 1
Epoch 25, global step 2158: 'val/ADE' was not in top 1
Epoch 26, global step 2241: 'val/ADE' was not in top 1
Epoch 27, global step 2324: 'val/ADE' was not in top 1
Epoch 28, global step 2407: 'val/ADE' was not in top 1
Epoch 29, global step 2490: 'val/ADE' was not in top 1
Epoch 30, global step 2573: 'val/ADE' was not in top 1
Epoch 31, global step 2656: 'val/ADE' was not in top 1
Epoch 32, global step 2739: 'val/ADE' was not in top 1
Epoch 33, global step 2822: 'val/ADE' was not in top 1
Epoch 34, global step 2905: 'val/ADE' was not in top 1
Monitored metric val/ADE = nan is not finite. Previous best value was 2.504. Signaling Trainer to stop.
Epoch 35, global step 2988: 'val/ADE' was not in top 1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
SLURM auto-requeueing enabled. Setting signal handlers.
wandb: - 155.097 MB of 155.097 MB uploadedwandb: \ 155.097 MB of 155.129 MB uploadedwandb: | 155.129 MB of 155.129 MB uploadedwandb: 
wandb: Run history:
wandb:   CPU energy consumption (kWh) â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–
wandb:   GPU energy consumption (kWh) â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–
wandb:   RAM energy consumption (kWh) â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–
wandb: Total energy consumption (kWh) â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–
wandb:                          epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:               train/loss_epoch â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚ 
wandb:                train/loss_step â–ˆâ–…â–…â–„â–‚â–†â–„â–†â–‚â–‚â–…â–„â–ƒâ–„â–„â–„â–…â–‚â–‚â–â–…â–„â–„â–ƒâ–„â–ƒâ–â–…â–…â–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒ 
wandb:            trainer/global_step â–â–â–â–‚â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–â–â–â–â–…â–…â–…â–…â–…â–†â–†â–†â–â–â–â–â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:                  val/ADE_epoch â–ˆâ–…â–…â–†â–†â–„â–„â–„â–„â–„â–„â–ƒâ–â–„â–†â–†â–…â–†â–…â–ƒâ–ƒâ–…â–„â–‡â–â–…â–†â–ƒâ–…â–ƒâ–…â–‡â–„â–…â–†  
wandb:                   val/ADE_step â–…â–…â–‡â–…â–„â–†â–ˆâ–ƒâ–…â–†â–„â–…â–‚â–â–…â–†â–„â–…â–†â–…â–„â–…â–…â–…â–…â–‡â–â–ƒâ–ƒâ–„â–ƒâ–…â–‡â–ˆâ–†â–ƒâ–‡â–…  
wandb:                  val/FDE_epoch â–ˆâ–…â–„â–†â–†â–„â–„â–„â–„â–„â–„â–ƒâ–â–„â–†â–†â–†â–‡â–…â–„â–ƒâ–…â–…â–‡â–â–…â–‡â–„â–†â–„â–…â–‡â–„â–…â–†  
wandb:                   val/FDE_step â–…â–„â–†â–†â–…â–†â–ˆâ–ƒâ–„â–†â–„â–…â–‚â–â–…â–…â–„â–„â–†â–…â–„â–…â–…â–…â–…â–†â–‚â–ƒâ–ƒâ–„â–ƒâ–†â–‡â–ˆâ–†â–ƒâ–†â–…  
wandb:               val/NL_ADE_epoch â–ˆâ–…â–…â–‡â–†â–ƒâ–…â–ƒâ–„â–ƒâ–„â–‚â–‚â–„â–…â–†â–†â–†â–„â–ƒâ–„â–…â–„â–ˆâ–â–„â–†â–„â–…â–ƒâ–„â–‡â–„â–„â–†  
wandb:                val/NL_ADE_step â–…â–…â–‡â–†â–„â–†â–†â–„â–„â–‡â–„â–…â–‚â–â–†â–…â–…â–„â–…â–…â–„â–…â–†â–…â–…â–ˆâ–‚â–‚â–„â–„â–ƒâ–„â–†â–ˆâ–…â–„â–‡â–…  
wandb: 
wandb: Run summary:
wandb:   CPU energy consumption (kWh) 4e-05
wandb:   GPU energy consumption (kWh) 0.00017
wandb:   RAM energy consumption (kWh) 0.0
wandb: Total energy consumption (kWh) 0.00021
wandb:                          epoch 36
wandb:               train/loss_epoch nan
wandb:                train/loss_step nan
wandb:            trainer/global_step 2988
wandb:                  val/ADE_epoch nan
wandb:                   val/ADE_step nan
wandb:                  val/FDE_epoch nan
wandb:                   val/FDE_step nan
wandb:               val/NL_ADE_epoch nan
wandb:                val/NL_ADE_step nan
wandb: 
wandb: ğŸš€ View run uni_lstm_SOC_train_50_100 at: https://wandb.ai/luffnis1/lightning_logs/runs/mg1gx42r
wandb: â­ï¸ View project at: https://wandb.ai/luffnis1/lightning_logs
wandb: Synced 6 W&B file(s), 0 media file(s), 9 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_163207-mg1gx42r/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
