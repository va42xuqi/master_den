/data/beegfs/home/gosalcds/miniconda3/envs/paper/lib/python3.11/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
wandb: Currently logged in as: luffnis (luffnis1). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in ./wandb/run-20240801_022559-k2onfkne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run uni_lstm_SOC_train_50_100_nba_fine_tune
wandb: ⭐️ View project at https://wandb.ai/luffnis1/lightning_logs
wandb: 🚀 View run at https://wandb.ai/luffnis1/lightning_logs/runs/k2onfkne
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name            | Type   | Params
-------------------------------------------
0 | x_preprocessing | LMU    | 199 K 
1 | y_preprocessing | LMU    | 199 K 
2 | x_lstm          | LSTM   | 526 K 
3 | y_lstm          | LSTM   | 526 K 
4 | x_fc_out        | Linear | 25.7 K
5 | y_fc_out        | Linear | 25.7 K
-------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.012     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val/ADE improved. New best score: 3.135
Epoch 0, global step 95: 'val/ADE' reached 3.13464 (best 3.13464), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_nba_fine_tune-v1.ckpt' as top 1
Metric val/ADE improved by 0.068 >= min_delta = 0.0. New best score: 3.067
Epoch 1, global step 190: 'val/ADE' reached 3.06710 (best 3.06710), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_nba_fine_tune-v1.ckpt' as top 1
Epoch 2, global step 285: 'val/ADE' was not in top 1
Metric val/ADE improved by 0.034 >= min_delta = 0.0. New best score: 3.033
Epoch 3, global step 380: 'val/ADE' reached 3.03271 (best 3.03271), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_nba_fine_tune-v1.ckpt' as top 1
Metric val/ADE improved by 0.067 >= min_delta = 0.0. New best score: 2.965
Epoch 4, global step 475: 'val/ADE' reached 2.96532 (best 2.96532), saving model to '/data/beegfs/home/gosalcds/master_den/badass_trajectory_predictor/checkpoints/soc/uni_lstm_nba_fine_tune-v1.ckpt' as top 1
Epoch 5, global step 570: 'val/ADE' was not in top 1
Monitored metric val/ADE = nan is not finite. Previous best value was 2.965. Signaling Trainer to stop.
Epoch 6, global step 665: 'val/ADE' was not in top 1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
SLURM auto-requeueing enabled. Setting signal handlers.
wandb: - 68.936 MB of 68.936 MB uploadedwandb: \ 68.936 MB of 68.936 MB uploadedwandb: | 68.936 MB of 68.936 MB uploadedwandb: / 68.945 MB of 68.974 MB uploaded (0.007 MB deduped)wandb: - 68.974 MB of 68.974 MB uploaded (0.007 MB deduped)wandb: 
wandb: Run history:
wandb:   CPU energy consumption (kWh) ▁▁▁▁▁▁▁█▁
wandb:   GPU energy consumption (kWh) ▁▁▁▁▁▁▁█▁
wandb:   RAM energy consumption (kWh) ▁▁▁▁▁▁▁█▁
wandb: Total energy consumption (kWh) ▁▁▁▁▁▁▁█▁
wandb:                          epoch ▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█
wandb:               train/loss_epoch █▄▆▅▅▁ 
wandb:                train/loss_step ▄▅▄▅▅█▅▆▄▃▅▆▄▅▁▄▅▁▇▃▄▆▄▃▆▅▆▇▇▆▆▆▅▄▄     
wandb:            trainer/global_step ▁▁▂▁▁▂▂▃▃▁▃▃▄▄▁▁▄▄▅▅▁▅▅▆▆▁▆▆▆▇▁▁▇▇█▁▁█▁█
wandb:                  val/ADE_epoch █▅█▄▁▃  
wandb:                   val/ADE_step ▅█▂▅▄▅▆▂▄▆▅█▄▃▅▅▆▁▃▆▄▆▁▄▃▅▅▂▃▆          
wandb:                  val/FDE_epoch █▅█▆▁▄  
wandb:                   val/FDE_step ▅█▂▄▅▅▆▂▃▆▅█▃▃▅▅▆▁▃▇▃▆▁▄▄▅▅▂▃▆          
wandb:               val/NL_ADE_epoch █▃█▂▁▃  
wandb:                val/NL_ADE_step ██▂▅▅▄▅▂▃▇▇█▄▂▅▃▆▁▂▆▅▆▁▂▄▅▅▂▃▅          
wandb: 
wandb: Run summary:
wandb:   CPU energy consumption (kWh) 3e-05
wandb:   GPU energy consumption (kWh) 0.00016
wandb:   RAM energy consumption (kWh) 0.0
wandb: Total energy consumption (kWh) 0.00019
wandb:                          epoch 7
wandb:               train/loss_epoch nan
wandb:                train/loss_step nan
wandb:            trainer/global_step 665
wandb:                  val/ADE_epoch nan
wandb:                   val/ADE_step nan
wandb:                  val/FDE_epoch nan
wandb:                   val/FDE_step nan
wandb:               val/NL_ADE_epoch nan
wandb:                val/NL_ADE_step nan
wandb: 
wandb: 🚀 View run uni_lstm_SOC_train_50_100_nba_fine_tune at: https://wandb.ai/luffnis1/lightning_logs/runs/k2onfkne
wandb: ⭐️ View project at: https://wandb.ai/luffnis1/lightning_logs
wandb: Synced 6 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240801_022559-k2onfkne/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
