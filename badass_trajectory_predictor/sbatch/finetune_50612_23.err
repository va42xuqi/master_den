/data/beegfs/home/gosalcds/miniconda3/envs/paper/lib/python3.11/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
wandb: Currently logged in as: luffnis (luffnis1). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in ./wandb/run-20240727_055929-xd6hj2go
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run oslstm_SOC_train_50_100
wandb: ‚≠êÔ∏è View project at https://wandb.ai/luffnis1/lightning_logs
wandb: üöÄ View run at https://wandb.ai/luffnis1/lightning_logs/runs/xd6hj2go
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name          | Type    | Params
------------------------------------------
0 | preprocessing | LMU     | 201 K 
1 | dropout_layer | Dropout | 0     
2 | lstm          | LSTM    | 526 K 
3 | fc_out        | Linear  | 51.4 K
------------------------------------------
779 K     Trainable params
0         Non-trainable params
779 K     Total params
3.117     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
